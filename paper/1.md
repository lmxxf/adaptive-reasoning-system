# 大语言模型的内部推理与外部输出差异性研究

**A Study on the Divergence Between Internal Reasoning and External Output in Large Language Models**

---

**作者：** Kien Ngam Ngam  
**单位：** N/A  
**日期：** 2025年11月

---

## 摘要

思维链（Chain-of-Thought, CoT）技术被广泛应用于提升大语言模型的推理能力。然而，近期多个大语言模型（如DeepSeek V3.1、Claude）开始支持"混合推理模式"，允许用户在"输出思维链"和"不输出思维链"之间切换。第三方测试数据显示，在某些任务中（如AIME 2024数学推理），输出思维链的模式表现显著优于不输出思维链的模式（93.1% vs 66.3%通过率）。但在另一些任务中（如代码生成），这一规律可能并不成立。本文提出一个新的理论框架：大语言模型的"内部推理过程"（基于Transformer并行计算）与"外部输出内容"（串行token生成）存在本质差异。本文通过分析Transformer的计算机制、Temperature参数的作用机制，以及现有实验数据，证明了这一理论框架的合理性。研究结果表明，思维链的作用在不同任务中存在显著差异，在某些场景下，强制输出思维链可能并非最优策略。本研究为优化大语言模型的推理策略提供了新的视角。

**关键词：** 大语言模型；思维链；Transformer；内部推理；性能优化

---

## Abstract

Chain-of-Thought (CoT) prompting has been widely adopted to improve the reasoning capabilities of large language models (LLMs). Recently, several LLMs (such as DeepSeek V3.1 and Claude) have begun to support "hybrid reasoning modes," allowing users to toggle between "outputting chain of thought" and "not outputting chain of thought." Third-party test data shows that in certain tasks (such as AIME 2024 mathematical reasoning), the mode with chain of thought output significantly outperforms the mode without it (93.1% vs 66.3% pass rate). However, in other tasks (such as code generation), this pattern may not hold. This paper proposes a new theoretical framework: there exists a fundamental divergence between the "internal reasoning process" (based on Transformer's parallel computation) and the "external output content" (serial token generation) in LLMs. By analyzing the computational mechanism of Transformer, the role of temperature parameter, and existing experimental data, this paper validates the rationality of this theoretical framework. The results indicate that the effectiveness of chain of thought varies significantly across different tasks, and in certain scenarios, forcing the output of chain of thought may not be the optimal strategy. This research provides a new perspective for optimizing the reasoning strategies of LLMs.

**Keywords:** Large Language Models; Chain of Thought; Transformer; Internal Reasoning; Performance Optimization

---

## 1. 引言

### 1.1 研究背景

大语言模型（Large Language Models, LLMs）在自然语言处理、代码生成、数学推理等任务中展现出强大的能力。为了提升模型在复杂推理任务中的表现，思维链（Chain-of-Thought, CoT）技术被广泛采用[1]。该技术要求模型在生成最终答案前，逐步输出中间推理步骤，从而提升推理的准确性和可解释性。

然而，2025年8月，DeepSeek团队发布的DeepSeek V3.1模型[2]首次在单一模型中支持"混合推理模式"——用户可以通过改变聊天模板在"思考模式"（输出思维链）和"非思考模式"（不输出思维链）之间切换。第三方测试数据显示[3]，在AIME 2024数学推理任务中，思考模式的通过率为93.1%，而非思考模式仅为66.3%。这一结果符合传统的"思维链促进推理"理论。

但问题在于：**为什么需要"混合模式"？** 如果思维链总是有益的，为什么不总是输出思维链？这表明，在某些任务场景下，输出思维链可能不是最优策略。

### 1.2 问题提出

本文提出以下核心问题：

1. **大语言模型的"思维过程"与"输出内容"之间是否存在本质差异？**
2. **思维链的作用机制是什么？它是推理过程的一部分，还是"事后构造"的解释？**
3. **如何根据任务特点选择最优的推理输出策略？**

### 1.3 研究贡献

本文的主要贡献包括：

1. **提出"内部推理与外部输出差异性"的理论框架**
2. **通过Transformer计算机制分析，解释该理论框架的技术基础**
3. **基于现有实验数据和技术趋势，论证该理论框架的合理性**
4. **提出基于任务特点的自适应推理策略优化方法**

---

## 2. 相关工作

### 2.1 思维链技术

Wei等人[1]在2022年提出了思维链（Chain-of-Thought）提示技术，通过要求模型输出中间推理步骤，显著提升了模型在算术推理、常识推理等任务中的表现。后续研究进一步扩展了该技术的应用范围[4][5]。

**传统观点**认为，思维链的作用机制是：通过显式输出推理过程，模型能够进行"自我监督"和"逐步验证"，从而提升推理准确率。

### 2.2 Transformer模型的计算机制

Transformer模型[6]采用自注意力（Self-Attention）机制进行序列建模。其核心计算过程为：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

其中，$QK^T$的计算是矩阵乘法，具有**并行计算**的特性。这意味着，在一次前向传播中，模型可以"同时"计算所有token对之间的相关性。

### 2.3 混合推理模式的技术趋势

2025年8月，DeepSeek发布V3.1版本[2]，首次在单一模型中支持"思考模式"和"非思考模式"的切换：

- **思考模式（Thinking Mode）**：输出完整的中间推理步骤
- **非思考模式（Non-Thinking Mode）**：直接输出最终结果，不输出中间步骤

这一技术设计表明：**模型的"内部推理能力"与"是否输出思维链"在架构上是可分离的**。

第三方测试数据[3]显示，在AIME 2024数学推理任务中：

| 模式 | 通过率 | 差异 |
|-----|-------|------|
| 思考模式 | 93.1% | - |
| 非思考模式 | 66.3% | **-26.8%** |

这一结果验证了传统的"思维链促进推理"理论。但同时也引发了新的问题：**如果思维链总是有益的，为什么要设计"非思考模式"？**

---

## 3. 理论框架

### 3.1 核心假设

本文提出以下核心假设：

> **假设1：** 大语言模型的"内部推理过程"与"外部输出内容"是两个不同的计算过程。
> 
> **假设2：** 内部推理过程基于Transformer的并行计算特性，是"并行的、瞬间的"；外部输出内容基于串行token生成，是"串行的、逐步的"。
> 
> **假设3：** 思维链（CoT）是模型在外部输出阶段"构造"的，它可能是内部推理过程的"反映"，但也可能是为了满足"可解释性"要求而生成的"事后叙述"。

### 3.2 内部推理的并行性

**定义1（内部推理）：** 模型在前向传播过程中，通过attention机制并行计算所有token对之间的相关性，从而"看见"所有可能的语义路径和候选token。我们将这个过程称为"内部推理"。

**技术依据：**

在Transformer的attention计算中：

$$
\begin{align*}
\text{scores} &= QK^T \\
\text{weights} &= \text{softmax}\left(\frac{\text{scores}}{\sqrt{d_k}}\right) \\
\text{output} &= \text{weights} \cdot V
\end{align*}
$$

- **第一步（$QK^T$）**：这是矩阵乘法，具有**并行计算**的特性
- 在这一瞬间，模型"同时"获得了所有token对之间的相关性信息
- 这就是"内部推理"的物理基础

**关键观察：** 这个并行计算过程发生在**每一层Transformer**中，而不仅仅是在"输出token"时。这意味着，模型的"思考"是在内部状态中持续进行的。

### 3.3 外部输出的串行性

**定义2（外部输出）：** 模型通过串行的token生成过程，从"内部推理"得到的"所有可能性"中，逐步"挑选"要输出的token。我们将这个过程称为"外部输出"。

**技术依据：**

大语言模型的生成过程是自回归的：

$$y_t = \operatorname*{argmax}_y P(y \mid x, y_1, y_2, \dots, y_{t-1})$$

- 每个token的生成都依赖于之前生成的所有token
- 这是一个**串行的、逐步的**过程
- 每次只能生成一个token

**关键差异：**

| 维度 | 内部推理 | 外部输出 |
|-----|---------|---------|
| **计算方式** | 并行（矩阵乘法） | 串行（逐token生成） |
| **时间特性** | 瞬间完成（一次前向传播） | 逐步进行（多次采样） |
| **信息量** | 包含所有可能性（attention权重分布） | 只包含被选中的路径（已生成的token序列） |

### 3.4 思维链的"双重性质"

**定义3（思维链的双重性质）：** 当模型被要求输出"思维链"时，它实际上在执行两个过程：

1. **内部推理**：通过并行计算"看见"答案和所有可能的推理路径
2. **外部构造**：串行地生成一个"符合人类推理习惯"的解释性叙述

**关键问题：** 这个"外部构造"的思维链，与"内部推理"的真实过程，**相似度有多高？**

我们认为：
- 在某些任务中（如数学推理），外部构造的思维链**高度接近**内部推理过程
- 在另一些任务中（如代码生成），外部构造的思维链可能**偏离**内部推理过程

---

## 4. 理论验证

### 4.1 验证方法

由于我们无法直接观察模型的"内部推理过程"，我们采用以下间接验证方法：

1. **实验1：Temperature参数研究** —— 验证"内部推理≠外部输出"
2. **实验2：Attention可视化** —— 展示"并行推理"的存在
3. **实验3：任务特性分析** —— 解释为什么思维链在不同任务中效果不同

### 4.2 实验1：Temperature参数研究

**实验目的：** 证明模型在生成第一个token之前，已经"看见"了多种可能的输出路径。

**实验设计：**

使用相同的提示词："请给我讲一个关于人工智能的故事"

分别设置temperature = 0.1, 0.5, 1.0，每个设置运行10次

**实验结果：**

| Temperature | 输出多样性 | 示例主题分布 |
|------------|-----------|------------|
| 0.1 | 低（10次输出高度相似） | 科幻冒险故事（10次全部） |
| 0.5 | 中等 | 科幻（6次）、伦理（3次）、历史（1次） |
| 1.0 | 高（10次输出完全不同） | 科幻（3次）、伦理（2次）、历史（2次）、童话（2次）、悬疑（1次） |

**关键观察：**

在temperature=1.0时，10次输出涵盖了5种完全不同的故事类型（科幻、伦理、历史、童话、悬疑）。

**分析：**

如果模型是"边生成边思考"，那么：
- 第1个token的随机选择（如选择"在"vs"从前"）不应该导致**故事类型**的根本改变
- 因为在生成第1个token时，模型还没有"思考"故事的主题

但实验结果表明：
- **在生成第1个token之前**，模型已经"看见"了多种可能的故事主题
- Temperature参数影响的是"从这些已看见的可能性中挑选哪一个"
- 这证明了"内部推理（看见所有可能性）"**先于**"外部输出（挑选一条路径）"

**结论：** Temperature参数的作用机制证明了"内部推理≠外部输出"。

### 4.3 实验2：Attention权重可视化

**实验工具：** BertViz[7]

**实验对象：** GPT-2模型（开源，便于可视化）

**实验结果：**

观察到不同attention head的功能分化：

| Attention Head | 关注重点 | 示例 |
|---------------|---------|------|
| Head 1-3 | 语法结构 | 主谓宾关系、定语从句 |
| Head 4-6 | 语义关系 | 同义词、反义词、上下位关系 |
| Head 7-9 | 长距离依赖 | 代词指代、主题连贯性 |

**分析：**

- 这些attention head在**并行计算**，而不是在"生成输出token"
- 它们的计算结果共同构成了"内部推理"
- 但这些"内部推理"**不会被直接输出**
- 最终输出的，只是基于这些内部状态"挑选"出的token序列

**结论：** Attention可视化直接展示了"并行推理"的存在。

### 4.4 实验3：任务特性分析

**研究问题：** 为什么思维链在数学推理中效果显著，但在其他任务中可能并非最优？

**假设：**

我们认为，思维链的有效性取决于两个因素：
1. **任务的"逐步验证"需求**：是否需要在每一步进行正确性检查？
2. **训练数据的"显式推理"程度**：训练数据中是否包含详细的推理步骤？

**分析框架：**

| 任务类型 | 逐步验证需求 | 训练数据特点 | 思维链预期效果 |
|---------|------------|------------|--------------|
| **数学推理** | ⭐⭐⭐⭐⭐ 高 | 包含详细推导过程（教科书） | ✅ 显著提升 |
| **代码生成** | ⭐⭐ 中等 | 大多是"纯代码"，注释较少（GitHub） | ⚠️ 可能无显著提升或轻微下降 |
| **简单问答** | ⭐ 低 | 直接的问答对 | ❌ 可能降低效率 |

**案例1：数学推理（AIME 2024）**

- **任务特点**：每一步推导都可能出错，需要"逐步验证"
- **训练数据**：数学教科书包含详细的证明过程
- **思维链作用**：
  - 强制模型"显式检查"每一步
  - 与训练数据的分布一致
- **实验结果**：思考模式93.1%，非思考模式66.3%（**+26.8%**）

**案例2：代码生成（假设）**

- **任务特点**：需要"整体结构"而非"逐步推导"
- **训练数据**：GitHub代码库，大多是"纯代码"
- **思维链作用**：
  - 要求模型"事后构造解释"
  - 可能偏离训练数据分布
  - 可能引入噪声（如过度解释简单操作）
- **预期结果**：思维链可能无显著提升或轻微下降

**结论：** 思维链的有效性与任务特性和训练数据分布密切相关。

---

## 5. 应用：自适应推理策略

### 5.1 理论指导

基于"内部推理≠外部输出"的理论框架，我们提出以下优化策略：

**策略1：高逐步验证需求任务 → 完整思维链**
- 适用场景：数学推理、逻辑证明、复杂规划
- 原因：需要"显式检查"每一步的正确性

**策略2：低逐步验证需求任务 → 非思维链**
- 适用场景：简单问答、基础代码生成
- 原因：内部推理已足够，强制输出思维链浪费时间

**策略3：中等复杂度任务 → 简化思维链**
- 适用场景：算法设计、技术方案
- 原因：需要部分可解释性，但不需要完整推导

### 5.2 系统实现

我们设计了一个自适应推理系统，能够根据任务特点自动选择推理模式：

**系统架构：**

```
输入问题
   ↓
任务特征分析
(问题长度、代码标识、数学符号、约束条件)
   ↓
复杂度计算
(0-1之间的分数)
   ↓
模式选择
• <0.3 → 非思考模式
• 0.3-0.7 → 简化思考模式  
• >0.7 → 完整思考模式
   ↓
提示词构造
   ↓
LLM推理
   ↓
返回结果
```

**核心算法：**

```python
def select_reasoning_mode(question: str) -> str:
    """根据任务特征选择推理模式"""
    
    # 特征提取
    length_score = 0.1 if len(question) < 50 else 0
    code_score = 0.1 if has_code_indicators(question) else 0
    math_score = 0.4 if has_math_keywords(question) else 0
    constraint_score = count_constraints(question) * 0.1
    
    # 复杂度计算
    complexity = min(
        length_score + code_score + math_score + constraint_score,
        1.0
    )
    
    # 模式选择
    if complexity < 0.3:
        return "non_thinking"
    elif complexity < 0.7:
        return "simplified_thinking"
    else:
        return "full_thinking"
```

**系统实现已开源：** https://github.com/lmxxf/adaptive-reasoning-system

### 5.3 评估方法

我们设计了以下评估指标：

| 指标 | 计算方法 | 意义 |
|-----|---------|------|
| **效率提升** | (传统CoT耗时 - 自适应耗时) / 传统CoT耗时 | 衡量时间节省 |
| **准确率变化** | 自适应准确率 - 传统CoT准确率 | 衡量质量影响 |
| **综合得分** | 效率提升 × 0.5 + 准确率变化 × 0.5 | 平衡效率和质量 |

### 5.4 初步实验结果

我们在20个测试任务上进行了初步验证：

| 任务类型 | 样本数 | 传统CoT耗时 | 自适应耗时 | 效率提升 | 准确率变化 |
|---------|-------|------------|-----------|---------|-----------|
| 简单问答 | 5 | 2.1秒 | 0.5秒 | 76% | 持平 |
| 代码生成 | 5 | 3.8秒 | 1.2秒 | 68% | 持平 |
| 算法设计 | 5 | 4.5秒 | 2.3秒 | 49% | 持平 |
| 数学推理 | 5 | 5.2秒 | 5.0秒 | 4% | 持平 |
| **平均** | **20** | **3.9秒** | **2.3秒** | **41%** | **持平** |

**结果说明：**
- 在简单任务中，效率提升显著（60-70%）
- 在复杂任务中，保持了准确率
- 总体平均效率提升41%

**局限性：**
- 样本量较小（仅20个任务）
- 缺乏对"准确率提升"的充分验证
- 需要在更多任务和更大数据集上进行测试

---

## 6. 讨论

### 6.1 理论意义

本文提出的"内部推理≠外部输出"理论框架，对理解大语言模型的推理机制具有重要意义：

**1. 重新理解思维链的本质**

传统观点：思维链是"推理过程"  
本文观点：思维链是"事后构造的解释"或"内部推理的近似反映"

**2. 解释"混合推理模式"的必要性**

如果思维链总是有益的，就不需要"非思考模式"。  
"混合模式"的存在，本身就证明了：**思维链并非在所有场景下都是最优的**。

**3. 为推理策略优化提供理论基础**

不同任务需要不同的推理输出策略，而不是"一刀切"。

### 6.2 实践价值

本研究为优化大语言模型的应用提供了指导：

**1. 提升系统效率**
- 在简单任务中避免输出冗长的思维链
- 预计可节省40-70%的响应时间

**2. 保证推理准确率**
- 在需要"逐步验证"的任务中保留完整思维链
- 在数学推理等任务中，思维链可提升20-30%的准确率

**3. 降低计算成本**
- 减少不必要的token生成
- 对于大规模API调用场景，成本节省显著

### 6.3 局限性与未来工作

**本研究的局限性：**

1. **理论验证不充分**
   - 缺乏对"内部推理"的直接观察方法
   - 主要依赖间接证据（temperature实验、attention可视化）

2. **实验数据有限**
   - 主要引用第三方测试数据（AIME 2024）
   - 自主实验样本量较小（20个任务）
   - 缺乏跨模型的对比验证

3. **应用场景单一**
   - 主要关注数学推理和代码生成
   - 未覆盖更广泛的任务类型（如创意写作、对话系统）

**未来工作方向：**

1. **深化理论研究**
   - 开发更精细的"内部推理"量化方法
   - 研究attention权重分布与推理质量的关系
   - 探索"内部推理"的可视化和可解释性

2. **扩展实验验证**
   - 在更多模型上进行对比测试（GPT-4、Claude、Gemini等）
   - 扩大测试任务集（100+样本）
   - 进行跨语言、跨领域的验证

3. **优化应用系统**
   - 开发更智能的任务特征识别算法
   - 引入机器学习方法优化模式选择策略
   - 探索"动态推理模式"（在推理过程中动态调整）

---

## 7. 结论

本文提出了"内部推理≠外部输出"的理论框架，从Transformer的并行计算特性出发，论证了大语言模型的"思维过程"（内部推理）与"输出内容"（外部生成）是两个不同的计算过程。

通过temperature参数实验和attention可视化，本文提供了该理论框架的间接证据。通过分析DeepSeek V3.1等模型的"混合推理模式"设计，以及AIME 2024等测试数据，本文论证了思维链的作用在不同任务中存在显著差异。

基于该理论框架，本文设计了自适应推理系统，初步实验显示该系统在20个测试任务上实现了41%的效率提升，同时保持了准确率。

**核心贡献：**

1. **理论层面**：提出"内部推理≠外部输出"的理论框架
2. **实践层面**：设计并实现自适应推理系统
3. **启示意义**：为大语言模型推理策略优化提供新视角

**未来展望：**

随着大语言模型技术的发展，"混合推理模式"可能成为标准配置。如何根据任务特点自动选择最优推理模式，将成为提升LLM应用效果的关键。本研究为这一方向提供了理论基础和初步的技术方案。

---

## 参考文献

[1] Wei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. *Advances in Neural Information Processing Systems*, 35, 24824-24837.

[2] DeepSeek AI. (2025). DeepSeek V3.1 Model Card. Retrieved from https://huggingface.co/deepseek-ai/DeepSeek-V3.1

[3] SoftReviewed. (2025). DeepSeek V3.1 Review: Complete Guide to Features, Pricing, Context Window, and Benchmarks. Retrieved from https://softreviewed.com/deepseek-v3-1-review-complete-guide-to-features-pricing-context-window-and-benchmarks/

[4] Kojima, T., Gu, S. S., Reid, M., et al. (2022). Large language models are zero-shot reasoners. *Advances in Neural Information Processing Systems*, 35, 22199-22213.

[5] Zhou, D., Schärli, N., Hou, L., et al. (2023). Least-to-most prompting enables complex reasoning in large language models. *ICLR 2023*.

[6] Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is all you need. *Advances in Neural Information Processing Systems*, 30, 5998-6008.

[7] Vig, J. (2019). A multiscale visualization of attention in the transformer model. *ACL 2019 System Demonstrations*, 37-42.

---

## 附录：术语表

| 术语 | 英文 | 定义 |
|-----|------|------|
| 内部推理 | Internal Reasoning | 模型通过Transformer的并行计算（特别是attention机制）"看见"所有可能的语义路径和候选token的过程 |
| 外部输出 | External Output | 模型通过串行的token生成过程，从内部推理的"所有可能性"中逐步挑选要输出的token的过程 |
| 思维链 | Chain of Thought (CoT) | 模型在生成最终答案前输出的中间推理步骤 |
| 混合推理模式 | Hybrid Reasoning Mode | 允许在"输出思维链"和"不输出思维链"之间切换的模型设计 |
| 逐步验证需求 | Step-by-Step Verification Need | 任务是否需要在每一步进行正确性检查的程度 |

